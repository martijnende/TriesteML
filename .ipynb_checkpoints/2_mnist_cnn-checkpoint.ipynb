{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced MNIST Example\n",
    "\n",
    "In the previous example, we naively squashed the 2D images into 1D vectors. By doing so, we lost some relevant information encoded in the spatial correlations between pixels. To include spatial correlations (or temporal correlations in timeseries and images sequences), one typically resorts to \"convolutional\" layers, which essentially scan the input for particular patterns.\n",
    "\n",
    "In this tutorial we will revisit the MNIST analysis with a convolutional neural network (CNN). The first steps are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Scale the input data\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST images are greyscale-valued, so they only have one channel. More generally, an image has 3 channels, so the model will expect an input size of $width \\times height \\times channels$. We therefore need to add one dimension to our train and test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, 3)\n",
    "test_images = np.expand_dims(test_images, 3)\n",
    "print(train_images.shape, test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model construction\n",
    "\n",
    "The procedure for constructing a convolutional neural network is the same as for a dense (fully-connected) neural network. Convolutional layers (`Conv2D`) can be added one-by-one, with pooling layers (`MaxPooling2D`) in between to condense the data size. With `Conv2D`, we have to specify the number of filters, the size of the kernel, the type of activation, and the rules handling boundaries. In the example below, each layer has 32 filters, a kernel size of 3x3 (`kernel_size=3`), and ReLU activation. The boundaries will be treated such that the size of the output is the same as the input (`padding=\"same\"`). After 3 convolutional layers, we squash the input and feed it into a single fully-connected layer with softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation=tf.nn.relu, padding=\"same\", input_shape=train_images[0].shape),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation=tf.nn.relu, padding=\"same\"),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(32, kernel_size=3, activation=tf.nn.relu, padding=\"same\"),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])\n",
    "\n",
    "# Compile and print a summary\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the total number of parameters is only a third of what we had in the case of only fully-connected layers, even though we have more layers. This is because convolutional layers \"reuse\" their parameters when going through the input data, and so the number of parameters does not depend on the size of the input. The number of parameters for a single convolutional layer can be calculated as $ \\left(K_w \\times K_l \\times C_{in} + 1 \\right) \\times C_{out}$, where $K_w$ and $K_l$ are the kernel width and length, $C_{in}$ is the number of input channels, and $C_{out}$ is the number of output channels. The $+1$ accounts for the biases that are added to each output channel. So for the first layer, we have $\\left(3 \\times 3 \\times 1 + 1 \\right) \\times 32 = 320$ parameters, and for the second and third layers we have $\\left(3 \\times 3 \\times 32 + 1 \\right) \\times 32 = 9248$ parameters each. The fully-connected layers takes an input of size $7 \\times 7 \\times 32 = 1568$, and produces an output of size $10$, so there are $1568 \\times 10 + 10 = 15,690$ weights and biases involved in the last layer.\n",
    "\n",
    "Because convolutional layers typically carry fewer parameters around than dense layers, you can keep stacking them at relatively low computational cost. This is what makes deep learning truly \"deep\". It is not uncommon to have CNN architectures with several tens of layers. \n",
    "\n",
    "## Training\n",
    "\n",
    "Let's see how our CNN architecture performs during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    validation_data=(test_images, test_labels),\n",
    "    verbose=1,\n",
    "    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "predictions = model.predict(test_images)\n",
    "print(\"Test accuracy: %.4f\" % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test accuracy of around 99% is much better than the 97% we had before. To put this into perspective: we first had an error rate of 3%, now it is only 1%, which is 3x less! Exploiting the spatial correlations using convolutional layers really helps (as you could have intuitively guessed).\n",
    "\n",
    "## Visualisation\n",
    "\n",
    "Again, we can visualise the performance by plotting the images and corresponding labels/predictions, but this time it will be even harder to find any miss-classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=10)\n",
    "for i in range(5):\n",
    "    for j in range(10):\n",
    "        n = i*10 + j\n",
    "        pred_num = np.argmax(predictions[n])\n",
    "        if pred_num == test_labels[n]:\n",
    "            colour = \"g\"\n",
    "        else:\n",
    "            colour = \"r\"\n",
    "        axes[i, j].set_title(\"%d / %d\" % (test_labels[n], pred_num), c=colour)\n",
    "        axes[i, j].imshow(test_images[n, :, :, 0], cmap=\"gray\")\n",
    "        axes[i, j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final note\n",
    "\n",
    "Using CNNs is not limited to image analysis. It works equally well for analysing 1D data like time-series, or 3D data like volumetric data or image sequences (movies). We will continue to use CNNs later when we analyse seismograms and GPS data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Just like in the previous tutorial, experiment with various hyperparameters: number of `Conv2D` layers, `kernel_size`, number of filters, activations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
